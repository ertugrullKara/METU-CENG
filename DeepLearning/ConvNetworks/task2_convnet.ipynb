{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a ConvNet!\n",
    "We now have a generic solver and a bunch of modularized layers. It's time to put it all together, and train a ConvNet to recognize the classes in CIFAR-10. In this notebook we will walk you through training a simple two-layer ConvNet and then set you free to build the best net that you can to perform well on CIFAR-10.\n",
    "\n",
    "Open up the file `cs231n/classifiers/convnet.py`; you will see that the `two_layer_convnet` function computes the loss and gradients for a two-layer ConvNet. Note that this function uses the \"sandwich\" layers defined in `cs231n/layer_utils.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifier_trainer import ClassifierTrainer\n",
    "from cs231n.gradient_check import eval_numerical_gradient\n",
    "from cs231n.classifiers.convnet import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-18, np.abs(x) + np.abs(y))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (19000, 3, 32, 32)\n",
      "Train labels shape:  (19000,)\n",
      "Validation data shape:  (1000, 3, 32, 32)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3, 32, 32)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "# Modify load_CIFAR10 and the following function to load less data if you have memory issues.\n",
    "# Load batches 1, 2 and 3; and call the function as follows:\n",
    "#def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "def get_CIFAR10_data(num_training=19000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    \n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
    "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
    "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check loss\n",
    "After you build a new network, one of the first things you should do is sanity check the loss. When we use the softmax loss, we expect the loss for random weights (and no regularization) to be about `log(C)` for `C` classes. When we add regularization this should go up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check loss (no regularization):  2.302517220161005\n",
      "Sanity check loss (with regularization):  2.3444527360543685\n"
     ]
    }
   ],
   "source": [
    "model = init_two_layer_convnet()\n",
    "\n",
    "X = np.random.randn(100, 3, 32, 32)\n",
    "y = np.random.randint(10, size=100)\n",
    "\n",
    "loss, _ = two_layer_convnet(X, model, y, reg=0)\n",
    "\n",
    "# Sanity check: Loss should be about log(10) = 2.3026\n",
    "print 'Sanity check loss (no regularization): ', loss\n",
    "\n",
    "# Sanity check: Loss should go up when you add regularization\n",
    "loss, _ = two_layer_convnet(X, model, y, reg=1)\n",
    "print 'Sanity check loss (with regularization): ', loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient check\n",
    "After the loss looks reasonable, you should always use numeric gradient checking to make sure that your backward pass is correct. When you use numeric gradient checking you should use a small amount of artifical data and a small number of neurons at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 max relative error: 1.577891e-06\n",
      "W2 max relative error: 1.731105e-05\n",
      "b1 max relative error: 1.351569e-07\n",
      "b2 max relative error: 2.277611e-07\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "input_shape = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "X = np.random.randn(num_inputs, *input_shape)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = init_two_layer_convnet(num_filters=3, filter_size=3, input_shape=input_shape)\n",
    "loss, grads = two_layer_convnet(X, model, y)\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: two_layer_convnet(X, model, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit small data\n",
    "A nice trick is to train your model with just a few training samples. You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting iteration  0\n",
      "Finished epoch 0 / 10: cost 2.305163, train: 0.120000, val 0.090000, lr 1.000000e-04\n",
      "Finished epoch 1 / 10: cost 2.222503, train: 0.320000, val 0.119000, lr 9.500000e-05\n",
      "Finished epoch 2 / 10: cost 1.925447, train: 0.400000, val 0.182000, lr 9.025000e-05\n",
      "starting iteration  10\n",
      "Finished epoch 3 / 10: cost 2.107890, train: 0.500000, val 0.174000, lr 8.573750e-05\n",
      "Finished epoch 4 / 10: cost 1.130183, train: 0.520000, val 0.142000, lr 8.145062e-05\n",
      "starting iteration  20\n",
      "Finished epoch 5 / 10: cost 0.713477, train: 0.640000, val 0.188000, lr 7.737809e-05\n",
      "Finished epoch 6 / 10: cost 0.902908, train: 0.780000, val 0.168000, lr 7.350919e-05\n",
      "starting iteration  30\n",
      "Finished epoch 7 / 10: cost 0.770443, train: 0.860000, val 0.177000, lr 6.983373e-05\n",
      "Finished epoch 8 / 10: cost 1.251094, train: 0.840000, val 0.189000, lr 6.634204e-05\n",
      "starting iteration  40\n",
      "Finished epoch 9 / 10: cost 0.665103, train: 0.940000, val 0.191000, lr 6.302494e-05\n",
      "Finished epoch 10 / 10: cost 0.038755, train: 1.000000, val 0.183000, lr 5.987369e-05\n",
      "finished optimization. best validation accuracy: 0.191000\n"
     ]
    }
   ],
   "source": [
    "# Use a two-layer ConvNet to overfit 50 training examples.\n",
    "\n",
    "model = init_two_layer_convnet()\n",
    "trainer = ClassifierTrainer()\n",
    "best_model, loss_history, train_acc_history, val_acc_history = trainer.train(\n",
    "          X_train[:50], y_train[:50], X_val, y_val, model, two_layer_convnet,\n",
    "          reg=0.001, momentum=0.9, learning_rate=0.0001, batch_size=10, num_epochs=10,\n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the loss, training accuracy, and validation accuracy should show clear overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f703fc7abd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_acc_history)\n",
    "plt.plot(val_acc_history)\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the net\n",
    "Once the above works, training the net is the next thing to try. You can set the `acc_frequency` parameter to change the frequency at which the training and validation set accuracies are tested. If your parameters are set properly, you should see the training and validation accuracy start to improve within a hundred iterations, and you should be able to train a reasonable model with just one epoch.\n",
    "\n",
    "Using the parameters below you should be able to get around 50% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting iteration  0\n",
      "Finished epoch 0 / 1: cost 2.305031, train: 0.066000, val 0.059000, lr 1.000000e-04\n",
      "starting iteration  10\n",
      "starting iteration  20\n",
      "starting iteration  30\n",
      "starting iteration  40\n",
      "starting iteration  50\n",
      "Finished epoch 0 / 1: cost 1.654591, train: 0.354000, val 0.356000, lr 1.000000e-04\n",
      "starting iteration  60\n",
      "starting iteration  70\n",
      "starting iteration  80\n",
      "starting iteration  90\n",
      "starting iteration  100\n",
      "Finished epoch 0 / 1: cost 1.781046, train: 0.331000, val 0.298000, lr 1.000000e-04\n",
      "starting iteration  110\n",
      "starting iteration  120\n",
      "starting iteration  130\n",
      "starting iteration  140\n",
      "starting iteration  150\n",
      "Finished epoch 0 / 1: cost 1.814826, train: 0.410000, val 0.374000, lr 1.000000e-04\n",
      "starting iteration  160\n",
      "starting iteration  170\n",
      "starting iteration  180\n",
      "starting iteration  190\n",
      "starting iteration  200\n",
      "Finished epoch 0 / 1: cost 1.849491, train: 0.428000, val 0.405000, lr 1.000000e-04\n",
      "starting iteration  210\n",
      "starting iteration  220\n",
      "starting iteration  230\n",
      "starting iteration  240\n",
      "starting iteration  250\n",
      "Finished epoch 0 / 1: cost 1.591758, train: 0.449000, val 0.430000, lr 1.000000e-04\n",
      "starting iteration  260\n",
      "starting iteration  270\n",
      "starting iteration  280\n",
      "starting iteration  290\n",
      "starting iteration  300\n",
      "Finished epoch 0 / 1: cost 1.419241, train: 0.408000, val 0.363000, lr 1.000000e-04\n",
      "starting iteration  310\n",
      "starting iteration  320\n",
      "starting iteration  330\n",
      "starting iteration  340\n",
      "starting iteration  350\n",
      "Finished epoch 0 / 1: cost 2.143002, train: 0.453000, val 0.437000, lr 1.000000e-04\n",
      "starting iteration  360\n",
      "starting iteration  370\n",
      "Finished epoch 1 / 1: cost 1.635849, train: 0.489000, val 0.441000, lr 9.500000e-05\n",
      "finished optimization. best validation accuracy: 0.441000\n"
     ]
    }
   ],
   "source": [
    "model = init_two_layer_convnet(filter_size=7)\n",
    "trainer = ClassifierTrainer()\n",
    "best_model, loss_history, train_acc_history, val_acc_history = trainer.train(\n",
    "          X_train, y_train, X_val, y_val, model, two_layer_convnet,\n",
    "          reg=0.001, momentum=0.9, learning_rate=0.0001, batch_size=50, num_epochs=1,\n",
    "          acc_frequency=50, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize weights\n",
    "We can visualize the convolutional weights from the first layer. If everything worked properly, these will usually be edges and blobs of various colors and orientations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f705d5df150>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f703fc7a750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cs231n.vis_utils import visualize_grid\n",
    "\n",
    "grid = visualize_grid(best_model['W1'].transpose(0, 2, 3, 1))\n",
    "plt.imshow(grid.astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment!\n",
    "Experiment and try to get the best performance that you can on CIFAR-10 using a ConvNet. Here are some ideas to get you started:\n",
    "\n",
    "### Things you should try:\n",
    "- Filter size: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- Number of filters: Above we used 32 filters. Do more or fewer do better?\n",
    "- Network depth: The network above has two layers of trainable parameters. Can you do better with a deeper network? You can implement alternative architectures in the file `cs231n/classifiers/convnet.py`. Some good architectures to try include:\n",
    "    - [conv-relu-pool]xN - conv - relu - [affine]xM - [softmax or SVM]\n",
    "    - [conv-relu-pool]XN - [affine]XM - [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN - [affine]xM - [softmax or SVM]\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the course-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum and RMSprop; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Other forms of regularization such as L1 or Dropout\n",
    "- Alternative activation functions such as leaky ReLU or maxout\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at least 65% accuracy on the validation set. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. The final cell in this notebook should contain the training, validation, and test set accuracies for your final trained network. In this notebook you should also write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network.\n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting iteration  0\n",
      "Finished epoch 0 / 2: cost 2.286587, train: 0.103000, val 0.088000, lr 1.000000e-04\n",
      "starting iteration  10\n",
      "starting iteration  20\n",
      "starting iteration  30\n",
      "starting iteration  40\n",
      "starting iteration  50\n",
      "starting iteration  60\n",
      "starting iteration  70\n",
      "starting iteration  80\n",
      "starting iteration  90\n",
      "starting iteration  100\n",
      "starting iteration  110\n",
      "starting iteration  120\n",
      "starting iteration  130\n",
      "starting iteration  140\n",
      "starting iteration  150\n",
      "starting iteration  160\n",
      "starting iteration  170\n",
      "starting iteration  180\n",
      "starting iteration  190\n",
      "starting iteration  200\n",
      "Finished epoch 0 / 2: cost 1.457822, train: 0.455000, val 0.433000, lr 1.000000e-04\n",
      "starting iteration  210\n",
      "starting iteration  220\n",
      "starting iteration  230\n",
      "starting iteration  240\n",
      "starting iteration  250\n",
      "starting iteration  260\n",
      "starting iteration  270\n",
      "starting iteration  280\n",
      "starting iteration  290\n",
      "starting iteration  300\n",
      "starting iteration  310\n",
      "starting iteration  320\n",
      "starting iteration  330\n",
      "starting iteration  340\n",
      "starting iteration  350\n",
      "starting iteration  360\n",
      "starting iteration  370\n",
      "Finished epoch 1 / 2: cost 1.445120, train: 0.516000, val 0.429000, lr 9.500000e-05\n",
      "starting iteration  380\n",
      "starting iteration  390\n",
      "starting iteration  400\n",
      "Finished epoch 1 / 2: cost 1.531425, train: 0.543000, val 0.475000, lr 9.500000e-05\n",
      "starting iteration  410\n",
      "starting iteration  420\n",
      "starting iteration  430\n",
      "starting iteration  440\n",
      "starting iteration  450\n",
      "starting iteration  460\n",
      "starting iteration  470\n",
      "starting iteration  480\n",
      "starting iteration  490\n",
      "starting iteration  500\n",
      "starting iteration  510\n",
      "starting iteration  520\n",
      "starting iteration  530\n",
      "starting iteration  540\n",
      "starting iteration  550\n",
      "starting iteration  560\n",
      "starting iteration  570\n",
      "starting iteration  580\n",
      "starting iteration  590\n",
      "starting iteration  600\n",
      "Finished epoch 1 / 2: cost 1.591236, train: 0.597000, val 0.505000, lr 9.500000e-05\n",
      "starting iteration  610\n",
      "starting iteration  620\n",
      "starting iteration  630\n",
      "starting iteration  640\n",
      "starting iteration  650\n",
      "starting iteration  660\n",
      "starting iteration  670\n",
      "starting iteration  680\n",
      "starting iteration  690\n",
      "starting iteration  700\n",
      "starting iteration  710\n",
      "starting iteration  720\n",
      "starting iteration  730\n",
      "starting iteration  740\n",
      "starting iteration  750\n",
      "Finished epoch 2 / 2: cost 1.144932, train: 0.576000, val 0.495000, lr 9.025000e-05\n",
      "finished optimization. best validation accuracy: 0.505000\n"
     ]
    }
   ],
   "source": [
    "# 2 Epochs 750 iteration 0.54Val\n",
    "model = init_two_layer_convnet(filter_size=5, num_filters=128)\n",
    "trainer = ClassifierTrainer()\n",
    "best_model, loss_history, train_acc_history, val_acc_history = trainer.train(\n",
    "          X_train, y_train, X_val, y_val, model, two_layer_convnet,\n",
    "          reg=0.001, momentum=0.9, learning_rate=0.0001, batch_size=50, num_epochs=2,\n",
    "          acc_frequency=200, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 0, 1, 'softmax', 50)\n",
      "starting iteration  0\n",
      "Finished epoch 0 / 2: cost 4.605369, train: 0.014000, val 0.016000, lr 4.000000e-04\n",
      "starting iteration  10\n",
      "starting iteration  20\n",
      "starting iteration  30\n",
      "starting iteration  40\n",
      "starting iteration  50\n",
      "starting iteration  60\n",
      "starting iteration  70\n",
      "starting iteration  80\n",
      "starting iteration  90\n",
      "starting iteration  100\n",
      "starting iteration  110\n",
      "starting iteration  120\n",
      "starting iteration  130\n",
      "starting iteration  140\n",
      "starting iteration  150\n",
      "starting iteration  160\n",
      "starting iteration  170\n",
      "starting iteration  180\n",
      "starting iteration  190\n",
      "starting iteration  200\n",
      "Finished epoch 0 / 2: cost 1.937659, train: 0.293000, val 0.268000, lr 4.000000e-04\n",
      "starting iteration  210\n",
      "starting iteration  220\n",
      "starting iteration  230\n",
      "starting iteration  240\n",
      "starting iteration  250\n",
      "starting iteration  260\n",
      "starting iteration  270\n",
      "starting iteration  280\n",
      "starting iteration  290\n",
      "starting iteration  300\n",
      "starting iteration  310\n",
      "starting iteration  320\n",
      "starting iteration  330\n",
      "starting iteration  340\n",
      "starting iteration  350\n",
      "starting iteration  360\n",
      "starting iteration  370\n",
      "Finished epoch 1 / 2: cost 1.471251, train: 0.389000, val 0.392000, lr 3.800000e-04\n",
      "starting iteration  380\n",
      "starting iteration  390\n",
      "starting iteration  400\n",
      "Finished epoch 1 / 2: cost 1.866073, train: 0.377000, val 0.383000, lr 3.800000e-04\n",
      "starting iteration  410\n",
      "starting iteration  420\n",
      "starting iteration  430\n",
      "starting iteration  440\n",
      "starting iteration  450\n",
      "starting iteration  460\n",
      "starting iteration  470\n",
      "starting iteration  480\n",
      "starting iteration  490\n",
      "starting iteration  500\n",
      "starting iteration  510\n",
      "starting iteration  520\n",
      "starting iteration  530\n",
      "starting iteration  540\n",
      "starting iteration  550\n",
      "starting iteration  560\n",
      "starting iteration  570\n",
      "starting iteration  580\n",
      "starting iteration  590\n",
      "starting iteration  600\n",
      "Finished epoch 1 / 2: cost 1.775489, train: 0.467000, val 0.465000, lr 3.800000e-04\n",
      "starting iteration  610\n",
      "starting iteration  620\n",
      "starting iteration  630\n",
      "starting iteration  640\n",
      "starting iteration  650\n",
      "starting iteration  660\n",
      "starting iteration  670\n",
      "starting iteration  680\n",
      "starting iteration  690\n",
      "starting iteration  700\n",
      "starting iteration  710\n",
      "starting iteration  720\n",
      "starting iteration  730\n",
      "starting iteration  740\n",
      "starting iteration  750\n",
      "Finished epoch 2 / 2: cost 1.376730, train: 0.497000, val 0.485000, lr 3.610000e-04\n",
      "finished optimization. best validation accuracy: 0.485000\n",
      "(3, 0, 2, 'softmax', 100)\n",
      "starting iteration  0\n",
      "Finished epoch 0 / 2: cost 4.605324, train: 0.103000, val 0.106000, lr 4.000000e-04\n",
      "starting iteration  10\n",
      "starting iteration  20\n",
      "starting iteration  30\n",
      "starting iteration  40\n",
      "starting iteration  50\n",
      "starting iteration  60\n",
      "starting iteration  70\n",
      "starting iteration  80\n",
      "starting iteration  90\n",
      "starting iteration  100\n",
      "starting iteration  110\n",
      "starting iteration  120\n",
      "starting iteration  130\n",
      "starting iteration  140\n",
      "starting iteration  150\n",
      "starting iteration  160\n",
      "starting iteration  170\n",
      "starting iteration  180\n",
      "Finished epoch 1 / 2: cost 4.540762, train: 0.103000, val 0.098000, lr 3.800000e-04\n",
      "starting iteration  190\n",
      "starting iteration  200\n",
      "Finished epoch 1 / 2: cost 4.537200, train: 0.108000, val 0.098000, lr 3.800000e-04\n",
      "starting iteration  210\n",
      "starting iteration  220\n",
      "starting iteration  230\n",
      "starting iteration  240\n",
      "starting iteration  250\n",
      "starting iteration  260\n",
      "starting iteration  270\n",
      "starting iteration  280\n",
      "starting iteration  290\n",
      "starting iteration  300\n",
      "starting iteration  310\n",
      "starting iteration  320\n",
      "starting iteration  330\n",
      "starting iteration  340\n",
      "starting iteration  350\n",
      "starting iteration  360\n",
      "starting iteration  370\n",
      "Finished epoch 2 / 2: cost 4.477020, train: 0.096000, val 0.101000, lr 3.610000e-04\n",
      "finished optimization. best validation accuracy: 0.106000\n",
      "(4, 0, 1, 'softmax', 50)\n",
      "starting iteration  0\n",
      "Finished epoch 0 / 2: cost 4.605316, train: 0.101000, val 0.104000, lr 4.000000e-04\n",
      "starting iteration  10\n",
      "starting iteration  20\n",
      "starting iteration  30\n",
      "starting iteration  40\n",
      "starting iteration  50\n",
      "starting iteration  60\n",
      "starting iteration  70\n",
      "starting iteration  80\n",
      "starting iteration  90\n",
      "starting iteration  100\n",
      "starting iteration  110\n",
      "starting iteration  120\n",
      "starting iteration  130\n",
      "starting iteration  140\n",
      "starting iteration  150\n",
      "starting iteration  160\n",
      "starting iteration  170\n",
      "starting iteration  180\n",
      "starting iteration  190\n",
      "starting iteration  200\n",
      "Finished epoch 0 / 2: cost 4.537609, train: 0.106000, val 0.101000, lr 4.000000e-04\n",
      "starting iteration  210\n",
      "starting iteration  220\n",
      "starting iteration  230\n",
      "starting iteration  240\n",
      "starting iteration  250\n",
      "starting iteration  260\n",
      "starting iteration  270\n",
      "starting iteration  280\n",
      "starting iteration  290\n",
      "starting iteration  300\n",
      "starting iteration  310\n",
      "starting iteration  320\n",
      "starting iteration  330\n",
      "starting iteration  340\n",
      "starting iteration  350\n",
      "starting iteration  360\n",
      "starting iteration  370\n",
      "Finished epoch 1 / 2: cost 4.473987, train: 0.099000, val 0.101000, lr 3.800000e-04\n",
      "starting iteration  380\n",
      "starting iteration  390\n",
      "starting iteration  400\n",
      "Finished epoch 1 / 2: cost 4.467246, train: 0.106000, val 0.101000, lr 3.800000e-04\n",
      "starting iteration  410\n",
      "starting iteration  420\n",
      "starting iteration  430\n",
      "starting iteration  440\n",
      "starting iteration  450\n",
      "starting iteration  460\n",
      "starting iteration  470\n",
      "starting iteration  480\n",
      "starting iteration  490\n",
      "starting iteration  500\n",
      "starting iteration  510\n",
      "starting iteration  520\n",
      "starting iteration  530\n",
      "starting iteration  540\n",
      "starting iteration  550\n",
      "starting iteration  560\n",
      "starting iteration  570\n",
      "starting iteration  580\n",
      "starting iteration  590\n",
      "starting iteration  600\n",
      "Finished epoch 1 / 2: cost 4.401093, train: 0.103000, val 0.101000, lr 3.800000e-04\n",
      "starting iteration  610\n",
      "starting iteration  620\n",
      "starting iteration  630\n",
      "starting iteration  640\n",
      "starting iteration  650\n",
      "starting iteration  660\n",
      "starting iteration  670\n",
      "starting iteration  680\n",
      "starting iteration  690\n",
      "starting iteration  700\n",
      "starting iteration  710\n",
      "starting iteration  720\n",
      "starting iteration  730\n",
      "starting iteration  740\n",
      "starting iteration  750\n",
      "Finished epoch 2 / 2: cost 4.348599, train: 0.101000, val 0.101000, lr 3.610000e-04\n",
      "finished optimization. best validation accuracy: 0.104000\n",
      "(4, 0, 1, 'softmax', 200)\n",
      "starting iteration  0\n",
      "Finished epoch 0 / 2: cost 4.605315, train: 0.097000, val 0.101000, lr 4.000000e-04\n",
      "starting iteration  10\n",
      "starting iteration  20\n",
      "starting iteration  30\n",
      "starting iteration  40\n",
      "starting iteration  50\n",
      "starting iteration  60\n",
      "starting iteration  70\n",
      "starting iteration  80\n",
      "starting iteration  90\n",
      "Finished epoch 1 / 2: cost 4.574819, train: 0.083000, val 0.101000, lr 3.800000e-04\n",
      "starting iteration  100\n",
      "starting iteration  110\n",
      "starting iteration  120\n",
      "starting iteration  130\n",
      "starting iteration  140\n",
      "starting iteration  150\n",
      "starting iteration  160\n",
      "starting iteration  170\n",
      "starting iteration  180\n",
      "Finished epoch 2 / 2: cost 4.542520, train: 0.090000, val 0.089000, lr 3.610000e-04\n",
      "finished optimization. best validation accuracy: 0.101000\n",
      "(4, 0, 0, 'softmax', 200)\n",
      "starting iteration  0\n",
      "Finished epoch 0 / 2: cost 19.408237, train: 0.000000, val 0.000000, lr 4.000000e-04\n",
      "starting iteration  10\n",
      "starting iteration  20\n",
      "starting iteration  30\n",
      "starting iteration  40\n",
      "starting iteration  50\n",
      "starting iteration  60\n",
      "starting iteration  70\n",
      "starting iteration  80\n",
      "starting iteration  90\n",
      "Finished epoch 1 / 2: cost 18.951594, train: 0.000000, val 0.000000, lr 3.800000e-04\n",
      "starting iteration  100\n",
      "starting iteration  110\n",
      "starting iteration  120\n",
      "starting iteration  130\n",
      "starting iteration  140\n",
      "starting iteration  150\n",
      "starting iteration  160\n",
      "starting iteration  170\n",
      "starting iteration  180\n",
      "Finished epoch 2 / 2: cost 18.467502, train: 0.000000, val 0.000000, lr 3.610000e-04\n",
      "finished optimization. best validation accuracy: 0.000000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'W0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2fb9946fb6b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m           acc_frequency=200, verbose=True)\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_convnet_imp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtest_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"train_acc:{}, val_acc:{}, test_acc:{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ertugrulcan46/env/HW2/cs231n/classifiers/convnet.pyc\u001b[0m in \u001b[0;36mmy_convnet_imp\u001b[0;34m(X, model, y, reg)\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconrelpool_layers\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_affine_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'W0'"
     ]
    }
   ],
   "source": [
    "filtersize=3\n",
    "numfilters=[32,64,128,128]\n",
    "k=0\n",
    "tried = []\n",
    "while k < 30:\n",
    "    num_crp = np.random.randint(1, 5)\n",
    "    num_crcrp = np.random.randint(1)\n",
    "    num_affine_layer = np.random.randint(3)\n",
    "#     loss = np.random.choice([\"softmax\", \"svm\"])\n",
    "    loss = \"softmax\"\n",
    "    batchsize = np.random.choice([50, 100, 200])\n",
    "    to_be_tried = (num_crp, num_crcrp, num_affine_layer, loss, batchsize)\n",
    "    if to_be_tried in tried:\n",
    "        continue\n",
    "    tried.append(to_be_tried)\n",
    "    print to_be_tried\n",
    "    output = \"num_crp:{}, num_crcrp:{}, num_aff:{}, batch_size:{}\\n\".format(str(num_crp), str(num_crcrp), str(num_affine_layer), str(batchsize))\n",
    "    model = init_my_convnet(filter_size=filtersize, num_filters=numfilters, \n",
    "                            num_crp=num_crp, num_affine_layer=num_affine_layer, num_crcrp=num_crcrp, loss=loss)\n",
    "    trainer = ClassifierTrainer()\n",
    "    best_model, loss_history, train_acc_history, val_acc_history = trainer.train(\n",
    "          X_train, y_train, X_val, y_val, model, my_convnet_imp,\n",
    "          reg=0.001, momentum=0.9, learning_rate=0.0004, batch_size=batchsize, num_epochs=2,\n",
    "          acc_frequency=200, verbose=True)\n",
    "    \n",
    "    test_predictions = my_convnet_imp(X_test, best_model).argmax(axis=1)\n",
    "    test_acc = np.mean(y_test == test_predictions)\n",
    "    output += \"train_acc:{}, val_acc:{}, test_acc:{}\\n\".format(str(max(train_acc_history)), str(max(val_acc_history)), str(test_acc))\n",
    "    with open(\"{}.txt\".format(str(k)), 'w') as f:\n",
    "        f.write(output)\n",
    "    np.save('{}.npy'.format(str(k)), best_model) # read_dictionary = np.load('my_file.npy').item()\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting iteration  0\n",
      "Finished epoch 0 / 15: cost 1.346730, train: 0.521000, val 0.492000, lr 5.000000e-04\n",
      "starting iteration  10\n",
      "starting iteration  20\n",
      "starting iteration  30\n",
      "starting iteration  40\n",
      "starting iteration  50\n",
      "Finished epoch 0 / 15: cost 1.441925, train: 0.495000, val 0.457000, lr 5.000000e-04\n",
      "starting iteration  60\n",
      "starting iteration  70\n",
      "starting iteration  80\n",
      "starting iteration  90\n",
      "starting iteration  100\n",
      "Finished epoch 0 / 15: cost 1.315123, train: 0.495000, val 0.526000, lr 5.000000e-04\n",
      "starting iteration  110\n",
      "starting iteration  120\n",
      "starting iteration  130\n",
      "starting iteration  140\n",
      "starting iteration  150\n",
      "Finished epoch 0 / 15: cost 1.128354, train: 0.508000, val 0.493000, lr 5.000000e-04\n",
      "starting iteration  160\n",
      "starting iteration  170\n",
      "starting iteration  180\n",
      "starting iteration  190\n",
      "starting iteration  200\n",
      "Finished epoch 0 / 15: cost 1.405125, train: 0.527000, val 0.508000, lr 5.000000e-04\n",
      "starting iteration  210\n",
      "starting iteration  220\n",
      "starting iteration  230\n",
      "starting iteration  240\n",
      "starting iteration  250\n",
      "Finished epoch 0 / 15: cost 1.440171, train: 0.536000, val 0.496000, lr 5.000000e-04\n",
      "starting iteration  260\n",
      "starting iteration  270\n",
      "starting iteration  280\n",
      "starting iteration  290\n",
      "starting iteration  300\n",
      "Finished epoch 0 / 15: cost 1.198557, train: 0.537000, val 0.509000, lr 5.000000e-04\n",
      "starting iteration  310\n",
      "starting iteration  320\n",
      "starting iteration  330\n",
      "starting iteration  340\n",
      "starting iteration  350\n",
      "Finished epoch 0 / 15: cost 1.246473, train: 0.559000, val 0.507000, lr 5.000000e-04\n",
      "starting iteration  360\n",
      "starting iteration  370\n",
      "Finished epoch 1 / 15: cost 1.002979, train: 0.587000, val 0.523000, lr 4.750000e-04\n",
      "starting iteration  380\n",
      "starting iteration  390\n",
      "starting iteration  400\n",
      "Finished epoch 1 / 15: cost 0.976297, train: 0.583000, val 0.522000, lr 4.750000e-04\n",
      "starting iteration  410\n",
      "starting iteration  420\n",
      "starting iteration  430\n",
      "starting iteration  440\n",
      "starting iteration  450\n",
      "Finished epoch 1 / 15: cost 1.194387, train: 0.556000, val 0.518000, lr 4.750000e-04\n",
      "starting iteration  460\n",
      "starting iteration  470\n",
      "starting iteration  480\n",
      "starting iteration  490\n",
      "starting iteration  500\n",
      "Finished epoch 1 / 15: cost 1.014610, train: 0.619000, val 0.558000, lr 4.750000e-04\n",
      "starting iteration  510\n",
      "starting iteration  520\n",
      "starting iteration  530\n",
      "starting iteration  540\n",
      "starting iteration  550\n",
      "Finished epoch 1 / 15: cost 1.228810, train: 0.588000, val 0.544000, lr 4.750000e-04\n",
      "starting iteration  560\n",
      "starting iteration  570\n",
      "starting iteration  580\n",
      "starting iteration  590\n",
      "starting iteration  600\n",
      "Finished epoch 1 / 15: cost 1.110240, train: 0.605000, val 0.509000, lr 4.750000e-04\n",
      "starting iteration  610\n",
      "starting iteration  620\n",
      "starting iteration  630\n",
      "starting iteration  640\n",
      "starting iteration  650\n",
      "Finished epoch 1 / 15: cost 0.804705, train: 0.625000, val 0.538000, lr 4.750000e-04\n",
      "starting iteration  660\n",
      "starting iteration  670\n",
      "starting iteration  680\n",
      "starting iteration  690\n",
      "starting iteration  700\n",
      "Finished epoch 1 / 15: cost 1.001352, train: 0.633000, val 0.561000, lr 4.750000e-04\n",
      "starting iteration  710\n",
      "starting iteration  720\n",
      "starting iteration  730\n",
      "starting iteration  740\n",
      "starting iteration  750\n",
      "Finished epoch 1 / 15: cost 0.768018, train: 0.638000, val 0.577000, lr 4.750000e-04\n",
      "Finished epoch 2 / 15: cost 0.971538, train: 0.626000, val 0.570000, lr 4.512500e-04\n",
      "starting iteration  760\n",
      "starting iteration  770\n",
      "starting iteration  780\n",
      "starting iteration  790\n",
      "starting iteration  800\n",
      "Finished epoch 2 / 15: cost 0.917883, train: 0.654000, val 0.586000, lr 4.512500e-04\n",
      "starting iteration  810\n",
      "starting iteration  820\n",
      "starting iteration  830\n",
      "starting iteration  840\n",
      "starting iteration  850\n",
      "Finished epoch 2 / 15: cost 0.917256, train: 0.624000, val 0.595000, lr 4.512500e-04\n",
      "starting iteration  860\n",
      "starting iteration  870\n",
      "starting iteration  880\n",
      "starting iteration  890\n",
      "starting iteration  900\n",
      "Finished epoch 2 / 15: cost 0.631594, train: 0.658000, val 0.561000, lr 4.512500e-04\n",
      "starting iteration  910\n",
      "starting iteration  920\n",
      "starting iteration  930\n",
      "starting iteration  940\n",
      "starting iteration  950\n",
      "Finished epoch 2 / 15: cost 0.947229, train: 0.676000, val 0.589000, lr 4.512500e-04\n",
      "starting iteration  960\n",
      "starting iteration  970\n",
      "starting iteration  980\n",
      "starting iteration  990\n",
      "starting iteration  1000\n",
      "Finished epoch 2 / 15: cost 1.100120, train: 0.633000, val 0.573000, lr 4.512500e-04\n",
      "starting iteration  1010\n",
      "starting iteration  1020\n",
      "starting iteration  1030\n",
      "starting iteration  1040\n",
      "starting iteration  1050\n",
      "Finished epoch 2 / 15: cost 0.781613, train: 0.639000, val 0.587000, lr 4.512500e-04\n",
      "starting iteration  1060\n",
      "starting iteration  1070\n",
      "starting iteration  1080\n",
      "starting iteration  1090\n",
      "starting iteration  1100\n",
      "Finished epoch 2 / 15: cost 1.077746, train: 0.635000, val 0.583000, lr 4.512500e-04\n",
      "starting iteration  1110\n",
      "starting iteration  1120\n",
      "starting iteration  1130\n",
      "Finished epoch 3 / 15: cost 0.923361, train: 0.688000, val 0.589000, lr 4.286875e-04\n",
      "starting iteration  1140\n",
      "starting iteration  1150\n",
      "Finished epoch 3 / 15: cost 1.126587, train: 0.692000, val 0.584000, lr 4.286875e-04\n",
      "starting iteration  1160\n",
      "starting iteration  1170\n",
      "starting iteration  1180\n",
      "starting iteration  1190\n",
      "starting iteration  1200\n",
      "Finished epoch 3 / 15: cost 0.777346, train: 0.670000, val 0.587000, lr 4.286875e-04\n",
      "starting iteration  1210\n",
      "starting iteration  1220\n",
      "starting iteration  1230\n",
      "starting iteration  1240\n",
      "starting iteration  1250\n",
      "Finished epoch 3 / 15: cost 0.969340, train: 0.660000, val 0.601000, lr 4.286875e-04\n",
      "starting iteration  1260\n",
      "starting iteration  1270\n",
      "starting iteration  1280\n",
      "starting iteration  1290\n",
      "starting iteration  1300\n",
      "Finished epoch 3 / 15: cost 0.908203, train: 0.673000, val 0.571000, lr 4.286875e-04\n",
      "starting iteration  1310\n",
      "starting iteration  1320\n",
      "starting iteration  1330\n",
      "starting iteration  1340\n",
      "starting iteration  1350\n",
      "Finished epoch 3 / 15: cost 0.818235, train: 0.689000, val 0.592000, lr 4.286875e-04\n",
      "starting iteration  1360\n",
      "starting iteration  1370\n",
      "starting iteration  1380\n",
      "starting iteration  1390\n",
      "starting iteration  1400\n",
      "Finished epoch 3 / 15: cost 0.962191, train: 0.695000, val 0.604000, lr 4.286875e-04\n",
      "starting iteration  1410\n",
      "starting iteration  1420\n",
      "starting iteration  1430\n",
      "starting iteration  1440\n",
      "starting iteration  1450\n",
      "Finished epoch 3 / 15: cost 0.466833, train: 0.690000, val 0.611000, lr 4.286875e-04\n",
      "starting iteration  1460\n",
      "starting iteration  1470\n",
      "starting iteration  1480\n",
      "starting iteration  1490\n",
      "starting iteration  1500\n",
      "Finished epoch 3 / 15: cost 0.462814, train: 0.707000, val 0.590000, lr 4.286875e-04\n",
      "starting iteration  1510\n",
      "Finished epoch 4 / 15: cost 0.701103, train: 0.741000, val 0.601000, lr 4.072531e-04\n",
      "starting iteration  1520\n",
      "starting iteration  1530\n",
      "starting iteration  1540\n",
      "starting iteration  1550\n",
      "Finished epoch 4 / 15: cost 0.915415, train: 0.694000, val 0.594000, lr 4.072531e-04\n",
      "starting iteration  1560\n",
      "starting iteration  1570\n",
      "starting iteration  1580\n",
      "starting iteration  1590\n",
      "starting iteration  1600\n",
      "Finished epoch 4 / 15: cost 0.780096, train: 0.691000, val 0.598000, lr 4.072531e-04\n",
      "starting iteration  1610\n",
      "starting iteration  1620\n",
      "starting iteration  1630\n",
      "starting iteration  1640\n",
      "starting iteration  1650\n",
      "Finished epoch 4 / 15: cost 0.943057, train: 0.715000, val 0.614000, lr 4.072531e-04\n",
      "starting iteration  1660\n",
      "starting iteration  1670\n",
      "starting iteration  1680\n",
      "starting iteration  1690\n",
      "starting iteration  1700\n",
      "Finished epoch 4 / 15: cost 0.888395, train: 0.749000, val 0.612000, lr 4.072531e-04\n",
      "starting iteration  1710\n",
      "starting iteration  1720\n",
      "starting iteration  1730\n",
      "starting iteration  1740\n",
      "starting iteration  1750\n",
      "Finished epoch 4 / 15: cost 0.641603, train: 0.682000, val 0.579000, lr 4.072531e-04\n",
      "starting iteration  1760\n",
      "starting iteration  1770\n",
      "starting iteration  1780\n",
      "starting iteration  1790\n",
      "starting iteration  1800\n",
      "Finished epoch 4 / 15: cost 0.570730, train: 0.726000, val 0.615000, lr 4.072531e-04\n",
      "starting iteration  1810\n",
      "starting iteration  1820\n",
      "starting iteration  1830\n",
      "starting iteration  1840\n",
      "starting iteration  1850\n",
      "Finished epoch 4 / 15: cost 0.754767, train: 0.719000, val 0.576000, lr 4.072531e-04\n",
      "starting iteration  1860\n",
      "starting iteration  1870\n",
      "starting iteration  1880\n",
      "starting iteration  1890\n",
      "Finished epoch 5 / 15: cost 0.773759, train: 0.733000, val 0.584000, lr 3.868905e-04\n",
      "starting iteration  1900\n",
      "Finished epoch 5 / 15: cost 0.722718, train: 0.729000, val 0.599000, lr 3.868905e-04\n",
      "starting iteration  1910\n",
      "starting iteration  1920\n",
      "starting iteration  1930\n",
      "starting iteration  1940\n",
      "starting iteration  1950\n",
      "Finished epoch 5 / 15: cost 0.873013, train: 0.738000, val 0.584000, lr 3.868905e-04\n",
      "starting iteration  1960\n",
      "starting iteration  1970\n",
      "starting iteration  1980\n",
      "starting iteration  1990\n",
      "starting iteration  2000\n",
      "Finished epoch 5 / 15: cost 0.785254, train: 0.754000, val 0.599000, lr 3.868905e-04\n",
      "starting iteration  2010\n",
      "starting iteration  2020\n",
      "starting iteration  2030\n",
      "starting iteration  2040\n",
      "starting iteration  2050\n",
      "Finished epoch 5 / 15: cost 0.631704, train: 0.742000, val 0.611000, lr 3.868905e-04\n",
      "starting iteration  2060\n",
      "starting iteration  2070\n",
      "starting iteration  2080\n",
      "starting iteration  2090\n",
      "starting iteration  2100\n",
      "Finished epoch 5 / 15: cost 0.904331, train: 0.755000, val 0.611000, lr 3.868905e-04\n",
      "starting iteration  2110\n",
      "starting iteration  2120\n",
      "starting iteration  2130\n",
      "starting iteration  2140\n",
      "starting iteration  2150\n",
      "Finished epoch 5 / 15: cost 0.602442, train: 0.758000, val 0.615000, lr 3.868905e-04\n",
      "starting iteration  2160\n",
      "starting iteration  2170\n",
      "starting iteration  2180\n",
      "starting iteration  2190\n",
      "starting iteration  2200\n",
      "Finished epoch 5 / 15: cost 0.655592, train: 0.770000, val 0.610000, lr 3.868905e-04\n",
      "starting iteration  2210\n",
      "starting iteration  2220\n",
      "starting iteration  2230\n",
      "starting iteration  2240\n",
      "starting iteration  2250\n",
      "Finished epoch 5 / 15: cost 0.589643, train: 0.786000, val 0.618000, lr 3.868905e-04\n",
      "starting iteration  2260\n",
      "starting iteration  2270\n",
      "Finished epoch 6 / 15: cost 1.001426, train: 0.781000, val 0.619000, lr 3.675459e-04\n",
      "starting iteration  2280\n",
      "starting iteration  2290\n",
      "starting iteration  2300\n",
      "Finished epoch 6 / 15: cost 0.773801, train: 0.729000, val 0.588000, lr 3.675459e-04\n",
      "starting iteration  2310\n",
      "starting iteration  2320\n",
      "starting iteration  2330\n",
      "starting iteration  2340\n",
      "starting iteration  2350\n",
      "Finished epoch 6 / 15: cost 0.847284, train: 0.762000, val 0.623000, lr 3.675459e-04\n",
      "starting iteration  2360\n",
      "starting iteration  2370\n",
      "starting iteration  2380\n",
      "starting iteration  2390\n",
      "starting iteration  2400\n",
      "Finished epoch 6 / 15: cost 0.902733, train: 0.747000, val 0.603000, lr 3.675459e-04\n",
      "starting iteration  2410\n",
      "starting iteration  2420\n",
      "starting iteration  2430\n",
      "starting iteration  2440\n",
      "starting iteration  2450\n",
      "Finished epoch 6 / 15: cost 0.911618, train: 0.792000, val 0.630000, lr 3.675459e-04\n",
      "starting iteration  2460\n",
      "starting iteration  2470\n",
      "starting iteration  2480\n",
      "starting iteration  2490\n",
      "starting iteration  2500\n",
      "Finished epoch 6 / 15: cost 0.977381, train: 0.742000, val 0.587000, lr 3.675459e-04\n",
      "starting iteration  2510\n",
      "starting iteration  2520\n",
      "starting iteration  2530\n",
      "starting iteration  2540\n",
      "starting iteration  2550\n",
      "Finished epoch 6 / 15: cost 0.619618, train: 0.788000, val 0.624000, lr 3.675459e-04\n",
      "starting iteration  2560\n",
      "starting iteration  2570\n",
      "starting iteration  2580\n",
      "starting iteration  2590\n",
      "starting iteration  2600\n",
      "Finished epoch 6 / 15: cost 0.575958, train: 0.756000, val 0.604000, lr 3.675459e-04\n",
      "starting iteration  2610\n",
      "starting iteration  2620\n",
      "starting iteration  2630\n",
      "starting iteration  2640\n",
      "starting iteration  2650\n",
      "Finished epoch 6 / 15: cost 0.492518, train: 0.792000, val 0.630000, lr 3.675459e-04\n",
      "Finished epoch 7 / 15: cost 0.510483, train: 0.791000, val 0.608000, lr 3.491686e-04\n",
      "starting iteration  2660\n",
      "starting iteration  2670\n",
      "starting iteration  2680\n",
      "starting iteration  2690\n",
      "starting iteration  2700\n",
      "Finished epoch 7 / 15: cost 0.687849, train: 0.784000, val 0.619000, lr 3.491686e-04\n",
      "starting iteration  2710\n",
      "starting iteration  2720\n",
      "starting iteration  2730\n",
      "starting iteration  2740\n",
      "starting iteration  2750\n",
      "Finished epoch 7 / 15: cost 0.787761, train: 0.771000, val 0.619000, lr 3.491686e-04\n",
      "starting iteration  2760\n",
      "starting iteration  2770\n",
      "starting iteration  2780\n",
      "starting iteration  2790\n",
      "starting iteration  2800\n",
      "Finished epoch 7 / 15: cost 0.549706, train: 0.792000, val 0.624000, lr 3.491686e-04\n",
      "starting iteration  2810\n",
      "starting iteration  2820\n",
      "starting iteration  2830\n",
      "starting iteration  2840\n",
      "starting iteration  2850\n",
      "Finished epoch 7 / 15: cost 0.732670, train: 0.798000, val 0.621000, lr 3.491686e-04\n",
      "starting iteration  2860\n",
      "starting iteration  2870\n",
      "starting iteration  2880\n",
      "starting iteration  2890\n",
      "starting iteration  2900\n",
      "Finished epoch 7 / 15: cost 0.843829, train: 0.805000, val 0.627000, lr 3.491686e-04\n",
      "starting iteration  2910\n",
      "starting iteration  2920\n",
      "starting iteration  2930\n",
      "starting iteration  2940\n",
      "starting iteration  2950\n",
      "Finished epoch 7 / 15: cost 0.732584, train: 0.762000, val 0.601000, lr 3.491686e-04\n",
      "starting iteration  2960\n",
      "starting iteration  2970\n",
      "starting iteration  2980\n",
      "starting iteration  2990\n",
      "starting iteration  3000\n",
      "Finished epoch 7 / 15: cost 0.677478, train: 0.791000, val 0.592000, lr 3.491686e-04\n",
      "starting iteration  3010\n",
      "starting iteration  3020\n",
      "starting iteration  3030\n",
      "Finished epoch 8 / 15: cost 0.652987, train: 0.797000, val 0.614000, lr 3.317102e-04\n",
      "starting iteration  3040\n",
      "starting iteration  3050\n",
      "Finished epoch 8 / 15: cost 0.408729, train: 0.828000, val 0.632000, lr 3.317102e-04\n",
      "starting iteration  3060\n",
      "starting iteration  3070\n",
      "starting iteration  3080\n",
      "starting iteration  3090\n",
      "starting iteration  3100\n",
      "Finished epoch 8 / 15: cost 0.520609, train: 0.796000, val 0.619000, lr 3.317102e-04\n",
      "starting iteration  3110\n",
      "starting iteration  3120\n",
      "starting iteration  3130\n",
      "starting iteration  3140\n",
      "starting iteration  3150\n",
      "Finished epoch 8 / 15: cost 0.483920, train: 0.823000, val 0.620000, lr 3.317102e-04\n",
      "starting iteration  3160\n",
      "starting iteration  3170\n",
      "starting iteration  3180\n",
      "starting iteration  3190\n",
      "starting iteration  3200\n",
      "Finished epoch 8 / 15: cost 0.665773, train: 0.818000, val 0.634000, lr 3.317102e-04\n",
      "starting iteration  3210\n",
      "starting iteration  3220\n",
      "starting iteration  3230\n",
      "starting iteration  3240\n",
      "starting iteration  3250\n",
      "Finished epoch 8 / 15: cost 0.504215, train: 0.819000, val 0.620000, lr 3.317102e-04\n",
      "starting iteration  3260\n",
      "starting iteration  3270\n",
      "starting iteration  3280\n",
      "starting iteration  3290\n",
      "starting iteration  3300\n",
      "Finished epoch 8 / 15: cost 0.598300, train: 0.836000, val 0.620000, lr 3.317102e-04\n",
      "starting iteration  3310\n",
      "starting iteration  3320\n",
      "starting iteration  3330\n",
      "starting iteration  3340\n",
      "starting iteration  3350\n",
      "Finished epoch 8 / 15: cost 0.582024, train: 0.810000, val 0.617000, lr 3.317102e-04\n",
      "starting iteration  3360\n",
      "starting iteration  3370\n",
      "starting iteration  3380\n",
      "starting iteration  3390\n",
      "starting iteration  3400\n",
      "Finished epoch 8 / 15: cost 0.546911, train: 0.853000, val 0.640000, lr 3.317102e-04\n",
      "starting iteration  3410\n",
      "Finished epoch 9 / 15: cost 0.491607, train: 0.837000, val 0.610000, lr 3.151247e-04\n",
      "starting iteration  3420\n",
      "starting iteration  3430\n",
      "starting iteration  3440\n",
      "starting iteration  3450\n",
      "Finished epoch 9 / 15: cost 0.631364, train: 0.804000, val 0.617000, lr 3.151247e-04\n",
      "starting iteration  3460\n",
      "starting iteration  3470\n",
      "starting iteration  3480\n",
      "starting iteration  3490\n",
      "starting iteration  3500\n",
      "Finished epoch 9 / 15: cost 0.430044, train: 0.835000, val 0.630000, lr 3.151247e-04\n",
      "starting iteration  3510\n",
      "starting iteration  3520\n",
      "starting iteration  3530\n",
      "starting iteration  3540\n",
      "starting iteration  3550\n",
      "Finished epoch 9 / 15: cost 0.423583, train: 0.841000, val 0.629000, lr 3.151247e-04\n",
      "starting iteration  3560\n",
      "starting iteration  3570\n",
      "starting iteration  3580\n",
      "starting iteration  3590\n",
      "starting iteration  3600\n",
      "Finished epoch 9 / 15: cost 0.349585, train: 0.837000, val 0.616000, lr 3.151247e-04\n",
      "starting iteration  3610\n",
      "starting iteration  3620\n",
      "starting iteration  3630\n",
      "starting iteration  3640\n",
      "starting iteration  3650\n",
      "Finished epoch 9 / 15: cost 0.623567, train: 0.855000, val 0.628000, lr 3.151247e-04\n",
      "starting iteration  3660\n",
      "starting iteration  3670\n",
      "starting iteration  3680\n",
      "starting iteration  3690\n",
      "starting iteration  3700\n",
      "Finished epoch 9 / 15: cost 0.448221, train: 0.843000, val 0.626000, lr 3.151247e-04\n",
      "starting iteration  3710\n",
      "starting iteration  3720\n",
      "starting iteration  3730\n",
      "starting iteration  3740\n",
      "starting iteration  3750\n",
      "Finished epoch 9 / 15: cost 0.608139, train: 0.855000, val 0.618000, lr 3.151247e-04\n",
      "starting iteration  3760\n",
      "starting iteration  3770\n",
      "starting iteration  3780\n",
      "starting iteration  3790\n",
      "Finished epoch 10 / 15: cost 0.561735, train: 0.871000, val 0.636000, lr 2.993685e-04\n",
      "starting iteration  3800\n",
      "Finished epoch 10 / 15: cost 0.533285, train: 0.858000, val 0.638000, lr 2.993685e-04\n",
      "starting iteration  3810\n",
      "starting iteration  3820\n",
      "starting iteration  3830\n",
      "starting iteration  3840\n",
      "starting iteration  3850\n",
      "Finished epoch 10 / 15: cost 1.091472, train: 0.834000, val 0.622000, lr 2.993685e-04\n",
      "starting iteration  3860\n",
      "starting iteration  3870\n",
      "starting iteration  3880\n",
      "starting iteration  3890\n",
      "starting iteration  3900\n",
      "Finished epoch 10 / 15: cost 0.437300, train: 0.831000, val 0.617000, lr 2.993685e-04\n",
      "starting iteration  3910\n",
      "starting iteration  3920\n",
      "starting iteration  3930\n",
      "starting iteration  3940\n",
      "starting iteration  3950\n",
      "Finished epoch 10 / 15: cost 0.637022, train: 0.863000, val 0.616000, lr 2.993685e-04\n",
      "starting iteration  3960\n",
      "starting iteration  3970\n",
      "starting iteration  3980\n",
      "starting iteration  3990\n",
      "starting iteration  4000\n",
      "Finished epoch 10 / 15: cost 0.457288, train: 0.855000, val 0.626000, lr 2.993685e-04\n",
      "starting iteration  4010\n",
      "starting iteration  4020\n",
      "starting iteration  4030\n",
      "starting iteration  4040\n",
      "starting iteration  4050\n",
      "Finished epoch 10 / 15: cost 0.406086, train: 0.859000, val 0.626000, lr 2.993685e-04\n",
      "starting iteration  4060\n",
      "starting iteration  4070\n",
      "starting iteration  4080\n",
      "starting iteration  4090\n",
      "starting iteration  4100\n",
      "Finished epoch 10 / 15: cost 0.352106, train: 0.843000, val 0.625000, lr 2.993685e-04\n",
      "starting iteration  4110\n",
      "starting iteration  4120\n",
      "starting iteration  4130\n",
      "starting iteration  4140\n",
      "starting iteration  4150\n",
      "Finished epoch 10 / 15: cost 0.371237, train: 0.847000, val 0.635000, lr 2.993685e-04\n",
      "starting iteration  4160\n",
      "starting iteration  4170\n",
      "Finished epoch 11 / 15: cost 0.438757, train: 0.830000, val 0.609000, lr 2.844000e-04\n",
      "starting iteration  4180\n",
      "starting iteration  4190\n",
      "starting iteration  4200\n",
      "Finished epoch 11 / 15: cost 0.425625, train: 0.837000, val 0.627000, lr 2.844000e-04\n",
      "starting iteration  4210\n",
      "starting iteration  4220\n",
      "starting iteration  4230\n",
      "starting iteration  4240\n",
      "starting iteration  4250\n",
      "Finished epoch 11 / 15: cost 0.349568, train: 0.879000, val 0.615000, lr 2.844000e-04\n",
      "starting iteration  4260\n",
      "starting iteration  4270\n",
      "starting iteration  4280\n",
      "starting iteration  4290\n",
      "starting iteration  4300\n",
      "Finished epoch 11 / 15: cost 0.510567, train: 0.874000, val 0.635000, lr 2.844000e-04\n",
      "starting iteration  4310\n",
      "starting iteration  4320\n",
      "starting iteration  4330\n",
      "starting iteration  4340\n",
      "starting iteration  4350\n",
      "Finished epoch 11 / 15: cost 0.593970, train: 0.872000, val 0.618000, lr 2.844000e-04\n",
      "starting iteration  4360\n",
      "starting iteration  4370\n",
      "starting iteration  4380\n",
      "starting iteration  4390\n",
      "starting iteration  4400\n",
      "Finished epoch 11 / 15: cost 0.373187, train: 0.850000, val 0.603000, lr 2.844000e-04\n",
      "starting iteration  4410\n",
      "starting iteration  4420\n",
      "starting iteration  4430\n",
      "starting iteration  4440\n",
      "starting iteration  4450\n",
      "Finished epoch 11 / 15: cost 0.597056, train: 0.856000, val 0.630000, lr 2.844000e-04\n",
      "starting iteration  4460\n",
      "starting iteration  4470\n",
      "starting iteration  4480\n",
      "starting iteration  4490\n",
      "starting iteration  4500\n",
      "Finished epoch 11 / 15: cost 0.498592, train: 0.877000, val 0.632000, lr 2.844000e-04\n",
      "starting iteration  4510\n",
      "starting iteration  4520\n",
      "starting iteration  4530\n",
      "starting iteration  4540\n",
      "starting iteration  4550\n",
      "Finished epoch 11 / 15: cost 0.294522, train: 0.872000, val 0.625000, lr 2.844000e-04\n",
      "Finished epoch 12 / 15: cost 0.373266, train: 0.894000, val 0.626000, lr 2.701800e-04\n",
      "starting iteration  4560\n",
      "starting iteration  4570\n",
      "starting iteration  4580\n",
      "starting iteration  4590\n",
      "starting iteration  4600\n",
      "Finished epoch 12 / 15: cost 0.387232, train: 0.870000, val 0.628000, lr 2.701800e-04\n",
      "starting iteration  4610\n",
      "starting iteration  4620\n",
      "starting iteration  4630\n",
      "starting iteration  4640\n",
      "starting iteration  4650\n",
      "Finished epoch 12 / 15: cost 0.384249, train: 0.878000, val 0.619000, lr 2.701800e-04\n",
      "starting iteration  4660\n",
      "starting iteration  4670\n",
      "starting iteration  4680\n",
      "starting iteration  4690\n",
      "starting iteration  4700\n",
      "Finished epoch 12 / 15: cost 0.280207, train: 0.887000, val 0.625000, lr 2.701800e-04\n",
      "starting iteration  4710\n",
      "starting iteration  4720\n",
      "starting iteration  4730\n",
      "starting iteration  4740\n",
      "starting iteration  4750\n",
      "Finished epoch 12 / 15: cost 0.460595, train: 0.873000, val 0.618000, lr 2.701800e-04\n",
      "starting iteration  4760\n",
      "starting iteration  4770\n",
      "starting iteration  4780\n",
      "starting iteration  4790\n",
      "starting iteration  4800\n",
      "Finished epoch 12 / 15: cost 0.484931, train: 0.873000, val 0.628000, lr 2.701800e-04\n",
      "starting iteration  4810\n",
      "starting iteration  4820\n",
      "starting iteration  4830\n",
      "starting iteration  4840\n",
      "starting iteration  4850\n",
      "Finished epoch 12 / 15: cost 0.304712, train: 0.868000, val 0.635000, lr 2.701800e-04\n",
      "starting iteration  4860\n",
      "starting iteration  4870\n",
      "starting iteration  4880\n",
      "starting iteration  4890\n",
      "starting iteration  4900\n",
      "Finished epoch 12 / 15: cost 0.136748, train: 0.885000, val 0.624000, lr 2.701800e-04\n",
      "starting iteration  4910\n",
      "starting iteration  4920\n",
      "starting iteration  4930\n",
      "Finished epoch 13 / 15: cost 0.421039, train: 0.877000, val 0.634000, lr 2.566710e-04\n",
      "starting iteration  4940\n",
      "starting iteration  4950\n",
      "Finished epoch 13 / 15: cost 0.292628, train: 0.881000, val 0.627000, lr 2.566710e-04\n",
      "starting iteration  4960\n",
      "starting iteration  4970\n",
      "starting iteration  4980\n",
      "starting iteration  4990\n",
      "starting iteration  5000\n",
      "Finished epoch 13 / 15: cost 0.269596, train: 0.891000, val 0.643000, lr 2.566710e-04\n",
      "starting iteration  5010\n",
      "starting iteration  5020\n",
      "starting iteration  5030\n",
      "starting iteration  5040\n",
      "starting iteration  5050\n",
      "Finished epoch 13 / 15: cost 0.418135, train: 0.901000, val 0.634000, lr 2.566710e-04\n",
      "starting iteration  5060\n",
      "starting iteration  5070\n",
      "starting iteration  5080\n",
      "starting iteration  5090\n",
      "starting iteration  5100\n",
      "Finished epoch 13 / 15: cost 0.321369, train: 0.895000, val 0.642000, lr 2.566710e-04\n",
      "starting iteration  5110\n",
      "starting iteration  5120\n",
      "starting iteration  5130\n",
      "starting iteration  5140\n",
      "starting iteration  5150\n",
      "Finished epoch 13 / 15: cost 0.359924, train: 0.887000, val 0.650000, lr 2.566710e-04\n",
      "starting iteration  5160\n",
      "starting iteration  5170\n",
      "starting iteration  5180\n",
      "starting iteration  5190\n",
      "starting iteration  5200\n",
      "Finished epoch 13 / 15: cost 0.385316, train: 0.874000, val 0.619000, lr 2.566710e-04\n",
      "starting iteration  5210\n",
      "starting iteration  5220\n",
      "starting iteration  5230\n",
      "starting iteration  5240\n",
      "starting iteration  5250\n",
      "Finished epoch 13 / 15: cost 0.412878, train: 0.890000, val 0.629000, lr 2.566710e-04\n",
      "starting iteration  5260\n",
      "starting iteration  5270\n",
      "starting iteration  5280\n",
      "starting iteration  5290\n",
      "starting iteration  5300\n",
      "Finished epoch 13 / 15: cost 0.197183, train: 0.881000, val 0.620000, lr 2.566710e-04\n",
      "starting iteration  5310\n",
      "Finished epoch 14 / 15: cost 0.241506, train: 0.894000, val 0.643000, lr 2.438375e-04\n",
      "starting iteration  5320\n",
      "starting iteration  5330\n",
      "starting iteration  5340\n",
      "starting iteration  5350\n",
      "Finished epoch 14 / 15: cost 0.176670, train: 0.906000, val 0.632000, lr 2.438375e-04\n",
      "starting iteration  5360\n",
      "starting iteration  5370\n",
      "starting iteration  5380\n",
      "starting iteration  5390\n",
      "starting iteration  5400\n",
      "Finished epoch 14 / 15: cost 0.430348, train: 0.906000, val 0.629000, lr 2.438375e-04\n",
      "starting iteration  5410\n",
      "starting iteration  5420\n",
      "starting iteration  5430\n",
      "starting iteration  5440\n",
      "starting iteration  5450\n",
      "Finished epoch 14 / 15: cost 0.474578, train: 0.914000, val 0.632000, lr 2.438375e-04\n",
      "starting iteration  5460\n",
      "starting iteration  5470\n",
      "starting iteration  5480\n",
      "starting iteration  5490\n",
      "starting iteration  5500\n",
      "Finished epoch 14 / 15: cost 0.271428, train: 0.899000, val 0.636000, lr 2.438375e-04\n",
      "starting iteration  5510\n",
      "starting iteration  5520\n",
      "starting iteration  5530\n",
      "starting iteration  5540\n",
      "starting iteration  5550\n",
      "Finished epoch 14 / 15: cost 0.235040, train: 0.911000, val 0.631000, lr 2.438375e-04\n",
      "starting iteration  5560\n",
      "starting iteration  5570\n",
      "starting iteration  5580\n",
      "starting iteration  5590\n",
      "starting iteration  5600\n",
      "Finished epoch 14 / 15: cost 0.217659, train: 0.918000, val 0.639000, lr 2.438375e-04\n",
      "starting iteration  5610\n",
      "starting iteration  5620\n",
      "starting iteration  5630\n",
      "starting iteration  5640\n",
      "starting iteration  5650\n",
      "Finished epoch 14 / 15: cost 0.175325, train: 0.908000, val 0.641000, lr 2.438375e-04\n",
      "starting iteration  5660\n",
      "starting iteration  5670\n",
      "starting iteration  5680\n",
      "starting iteration  5690\n",
      "Finished epoch 15 / 15: cost 0.259761, train: 0.921000, val 0.635000, lr 2.316456e-04\n",
      "finished optimization. best validation accuracy: 0.650000\n"
     ]
    }
   ],
   "source": [
    "init_my_convnet(filter_size=filtersize, num_filters=numfilters, \n",
    "                            num_crp=2, num_affine_layer=1, num_crcrp=0, loss=loss)\n",
    "model = np.load('0.npy').item()\n",
    "trainer = ClassifierTrainer()\n",
    "best_model, loss_history, train_acc_history, val_acc_history = trainer.train(\n",
    "          X_train, y_train, X_val, y_val, model, my_convnet_imp,\n",
    "          reg=0.001, momentum=0.9, learning_rate=0.0005, batch_size=50, num_epochs=15,\n",
    "          acc_frequency=50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.647\n"
     ]
    }
   ],
   "source": [
    "test_predictions = my_convnet_imp(X_test, best_model).argmax(axis=1)\n",
    "test_acc = np.mean(y_test == test_predictions)\n",
    "print test_acc\n",
    "best_model_1 = best_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax 2 2 0\n",
      "starting iteration  0\n",
      "Finished epoch 0 / 5: cost 6.910350, train: 0.059000, val 0.066000, lr 5.000000e-04\n",
      "starting iteration  10\n",
      "starting iteration  20\n",
      "starting iteration  30\n",
      "starting iteration  40\n",
      "starting iteration  50\n",
      "Finished epoch 0 / 5: cost 6.889845, train: 0.087000, val 0.101000, lr 5.000000e-04\n",
      "starting iteration  60\n",
      "starting iteration  70\n",
      "starting iteration  80\n",
      "starting iteration  90\n",
      "starting iteration  100\n",
      "Finished epoch 0 / 5: cost 6.865714, train: 0.111000, val 0.101000, lr 5.000000e-04\n",
      "starting iteration  110\n",
      "starting iteration  120\n",
      "starting iteration  130\n",
      "starting iteration  140\n",
      "starting iteration  150\n",
      "Finished epoch 0 / 5: cost 6.840805, train: 0.095000, val 0.101000, lr 5.000000e-04\n",
      "starting iteration  160\n",
      "starting iteration  170\n",
      "starting iteration  180\n",
      "starting iteration  190\n",
      "starting iteration  200\n",
      "Finished epoch 0 / 5: cost 6.815381, train: 0.109000, val 0.098000, lr 5.000000e-04\n",
      "starting iteration  210\n",
      "starting iteration  220\n",
      "starting iteration  230\n",
      "starting iteration  240\n",
      "starting iteration  250\n",
      "Finished epoch 0 / 5: cost 6.789984, train: 0.110000, val 0.098000, lr 5.000000e-04\n",
      "starting iteration  260\n",
      "starting iteration  270\n",
      "starting iteration  280\n",
      "starting iteration  290\n",
      "starting iteration  300\n",
      "Finished epoch 0 / 5: cost 6.734726, train: 0.123000, val 0.122000, lr 5.000000e-04\n",
      "starting iteration  310\n",
      "starting iteration  320\n",
      "starting iteration  330\n",
      "starting iteration  340\n",
      "starting iteration  350\n",
      "Finished epoch 0 / 5: cost 6.733740, train: 0.092000, val 0.100000, lr 5.000000e-04\n",
      "starting iteration  360\n",
      "starting iteration  370\n",
      "Finished epoch 1 / 5: cost 3.925187, train: 0.093000, val 0.098000, lr 4.750000e-04\n",
      "starting iteration  380\n",
      "starting iteration  390\n",
      "starting iteration  400\n",
      "Finished epoch 1 / 5: cost 2.513496, train: 0.154000, val 0.152000, lr 4.750000e-04\n",
      "starting iteration  410\n",
      "starting iteration  420\n",
      "starting iteration  430\n",
      "starting iteration  440\n",
      "starting iteration  450\n",
      "Finished epoch 1 / 5: cost 2.349633, train: 0.160000, val 0.167000, lr 4.750000e-04\n",
      "starting iteration  460\n",
      "starting iteration  470\n",
      "starting iteration  480\n",
      "starting iteration  490\n",
      "starting iteration  500\n",
      "Finished epoch 1 / 5: cost 2.211407, train: 0.201000, val 0.202000, lr 4.750000e-04\n",
      "starting iteration  510\n",
      "starting iteration  520\n",
      "starting iteration  530\n",
      "starting iteration  540\n",
      "starting iteration  550\n",
      "Finished epoch 1 / 5: cost 2.015376, train: 0.233000, val 0.251000, lr 4.750000e-04\n",
      "starting iteration  560\n",
      "starting iteration  570\n",
      "starting iteration  580\n",
      "starting iteration  590\n",
      "starting iteration  600\n",
      "Finished epoch 1 / 5: cost 1.765023, train: 0.278000, val 0.280000, lr 4.750000e-04\n",
      "starting iteration  610\n",
      "starting iteration  620\n",
      "starting iteration  630\n",
      "starting iteration  640\n",
      "starting iteration  650\n",
      "Finished epoch 1 / 5: cost 2.054006, train: 0.340000, val 0.306000, lr 4.750000e-04\n",
      "starting iteration  660\n",
      "starting iteration  670\n",
      "starting iteration  680\n",
      "starting iteration  690\n",
      "starting iteration  700\n",
      "Finished epoch 1 / 5: cost 1.939253, train: 0.251000, val 0.281000, lr 4.750000e-04\n",
      "starting iteration  710\n",
      "starting iteration  720\n",
      "starting iteration  730\n",
      "starting iteration  740\n",
      "starting iteration  750\n",
      "Finished epoch 1 / 5: cost 1.708858, train: 0.320000, val 0.338000, lr 4.750000e-04\n",
      "Finished epoch 2 / 5: cost 1.784483, train: 0.341000, val 0.337000, lr 4.512500e-04\n",
      "starting iteration  760\n",
      "starting iteration  770\n",
      "starting iteration  780\n",
      "starting iteration  790\n",
      "starting iteration  800\n",
      "Finished epoch 2 / 5: cost 1.900246, train: 0.369000, val 0.371000, lr 4.512500e-04\n",
      "starting iteration  810\n",
      "starting iteration  820\n",
      "starting iteration  830\n",
      "starting iteration  840\n",
      "starting iteration  850\n",
      "Finished epoch 2 / 5: cost 1.916141, train: 0.383000, val 0.391000, lr 4.512500e-04\n",
      "starting iteration  860\n",
      "starting iteration  870\n",
      "starting iteration  880\n",
      "starting iteration  890\n",
      "starting iteration  900\n",
      "Finished epoch 2 / 5: cost 1.811316, train: 0.370000, val 0.373000, lr 4.512500e-04\n",
      "starting iteration  910\n",
      "starting iteration  920\n",
      "starting iteration  930\n",
      "starting iteration  940\n",
      "starting iteration  950\n",
      "Finished epoch 2 / 5: cost 1.557081, train: 0.401000, val 0.400000, lr 4.512500e-04\n",
      "starting iteration  960\n",
      "starting iteration  970\n",
      "starting iteration  980\n",
      "starting iteration  990\n",
      "starting iteration  1000\n",
      "Finished epoch 2 / 5: cost 1.492783, train: 0.412000, val 0.389000, lr 4.512500e-04\n",
      "starting iteration  1010\n",
      "starting iteration  1020\n",
      "starting iteration  1030\n",
      "starting iteration  1040\n",
      "starting iteration  1050\n",
      "Finished epoch 2 / 5: cost 1.730070, train: 0.425000, val 0.419000, lr 4.512500e-04\n",
      "starting iteration  1060\n",
      "starting iteration  1070\n",
      "starting iteration  1080\n",
      "starting iteration  1090\n",
      "starting iteration  1100\n",
      "Finished epoch 2 / 5: cost 1.601019, train: 0.434000, val 0.415000, lr 4.512500e-04\n",
      "starting iteration  1110\n",
      "starting iteration  1120\n",
      "starting iteration  1130\n",
      "Finished epoch 3 / 5: cost 1.762098, train: 0.441000, val 0.412000, lr 4.286875e-04\n",
      "starting iteration  1140\n",
      "starting iteration  1150\n",
      "Finished epoch 3 / 5: cost 1.192580, train: 0.444000, val 0.423000, lr 4.286875e-04\n",
      "starting iteration  1160\n",
      "starting iteration  1170\n",
      "starting iteration  1180\n",
      "starting iteration  1190\n",
      "starting iteration  1200\n",
      "Finished epoch 3 / 5: cost 1.802347, train: 0.451000, val 0.438000, lr 4.286875e-04\n",
      "starting iteration  1210\n",
      "starting iteration  1220\n",
      "starting iteration  1230\n",
      "starting iteration  1240\n",
      "starting iteration  1250\n",
      "Finished epoch 3 / 5: cost 1.474968, train: 0.433000, val 0.444000, lr 4.286875e-04\n",
      "starting iteration  1260\n",
      "starting iteration  1270\n",
      "starting iteration  1280\n",
      "starting iteration  1290\n",
      "starting iteration  1300\n",
      "Finished epoch 3 / 5: cost 1.826210, train: 0.447000, val 0.425000, lr 4.286875e-04\n",
      "starting iteration  1310\n",
      "starting iteration  1320\n",
      "starting iteration  1330\n",
      "starting iteration  1340\n",
      "starting iteration  1350\n",
      "Finished epoch 3 / 5: cost 1.296094, train: 0.486000, val 0.470000, lr 4.286875e-04\n",
      "starting iteration  1360\n",
      "starting iteration  1370\n",
      "starting iteration  1380\n",
      "starting iteration  1390\n",
      "starting iteration  1400\n",
      "Finished epoch 3 / 5: cost 1.550197, train: 0.482000, val 0.473000, lr 4.286875e-04\n",
      "starting iteration  1410\n",
      "starting iteration  1420\n",
      "starting iteration  1430\n",
      "starting iteration  1440\n",
      "starting iteration  1450\n",
      "Finished epoch 3 / 5: cost 1.312696, train: 0.473000, val 0.471000, lr 4.286875e-04\n",
      "starting iteration  1460\n",
      "starting iteration  1470\n",
      "starting iteration  1480\n",
      "starting iteration  1490\n",
      "starting iteration  1500\n",
      "Finished epoch 3 / 5: cost 1.359664, train: 0.497000, val 0.472000, lr 4.286875e-04\n",
      "starting iteration  1510\n",
      "Finished epoch 4 / 5: cost 1.344442, train: 0.504000, val 0.482000, lr 4.072531e-04\n",
      "starting iteration  1520\n",
      "starting iteration  1530\n",
      "starting iteration  1540\n",
      "starting iteration  1550\n",
      "Finished epoch 4 / 5: cost 1.330201, train: 0.512000, val 0.475000, lr 4.072531e-04\n",
      "starting iteration  1560\n",
      "starting iteration  1570\n",
      "starting iteration  1580\n",
      "starting iteration  1590\n",
      "starting iteration  1600\n",
      "Finished epoch 4 / 5: cost 1.291100, train: 0.524000, val 0.481000, lr 4.072531e-04\n",
      "starting iteration  1610\n",
      "starting iteration  1620\n",
      "starting iteration  1630\n",
      "starting iteration  1640\n",
      "starting iteration  1650\n",
      "Finished epoch 4 / 5: cost 1.381240, train: 0.519000, val 0.505000, lr 4.072531e-04\n",
      "starting iteration  1660\n",
      "starting iteration  1670\n",
      "starting iteration  1680\n",
      "starting iteration  1690\n",
      "starting iteration  1700\n",
      "Finished epoch 4 / 5: cost 1.082572, train: 0.526000, val 0.507000, lr 4.072531e-04\n",
      "starting iteration  1710\n",
      "starting iteration  1720\n",
      "starting iteration  1730\n",
      "starting iteration  1740\n",
      "starting iteration  1750\n",
      "Finished epoch 4 / 5: cost 1.106947, train: 0.501000, val 0.521000, lr 4.072531e-04\n",
      "starting iteration  1760\n",
      "starting iteration  1770\n",
      "starting iteration  1780\n",
      "starting iteration  1790\n",
      "starting iteration  1800\n",
      "Finished epoch 4 / 5: cost 1.225987, train: 0.510000, val 0.519000, lr 4.072531e-04\n",
      "starting iteration  1810\n",
      "starting iteration  1820\n",
      "starting iteration  1830\n",
      "starting iteration  1840\n",
      "starting iteration  1850\n",
      "Finished epoch 4 / 5: cost 1.384267, train: 0.538000, val 0.514000, lr 4.072531e-04\n",
      "starting iteration  1860\n",
      "starting iteration  1870\n",
      "starting iteration  1880\n",
      "starting iteration  1890\n",
      "Finished epoch 5 / 5: cost 1.248679, train: 0.525000, val 0.538000, lr 3.868905e-04\n",
      "finished optimization. best validation accuracy: 0.538000\n"
     ]
    }
   ],
   "source": [
    "model = init_my_convnet(filter_size=5, num_filters=[64, 64, 128, 128, 128], num_crp=2, num_affine_layer=2, num_crcrp=0)\n",
    "trainer = ClassifierTrainer()\n",
    "best_model, loss_history, train_acc_history, val_acc_history = trainer.train(\n",
    "          X_train, y_train, X_val, y_val, model, my_convnet_imp,\n",
    "          reg=0.001, momentum=0.9, learning_rate=0.0005, batch_size=50, num_epochs=5,\n",
    "          acc_frequency=50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.526\n"
     ]
    }
   ],
   "source": [
    "test_predictions = my_convnet_imp(X_test, best_model).argmax(axis=1)\n",
    "test_acc = np.mean(y_test == test_predictions)\n",
    "print test_acc\n",
    "best_model_2 = best_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax 1 4 0\n",
      "starting iteration  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-48795af36917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_convnet_imp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           acc_frequency=50, verbose=True)\n\u001b[0m",
      "\u001b[0;32m/home/ertugrulcan46/env/HW2/cs231n/classifier_trainer.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, X_val, y_val, model, loss_function, reg, learning_rate, momentum, learning_rate_decay, update, sample_batches, num_epochs, batch_size, acc_frequency, verbose)\u001b[0m\n\u001b[1;32m    136\u001b[0m           \u001b[0mX_train_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m           \u001b[0my_train_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mscores_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_train_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ertugrulcan46/env/HW2/cs231n/classifiers/convnet.py\u001b[0m in \u001b[0;36mmy_convnet_imp\u001b[0;34m(X, model, y, reg)\u001b[0m\n\u001b[1;32m    151\u001b[0m       \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mlayer_num\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0ma_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_relu_pool_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ertugrulcan46/env/HW2/cs231n/layer_utils.pyc\u001b[0m in \u001b[0;36mconv_relu_pool_forward\u001b[0;34m(x, w, b, conv_param, pool_param)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;34m-\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mObject\u001b[0m \u001b[0mto\u001b[0m \u001b[0mgive\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_forward_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m   \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_pool_forward_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ertugrulcan46/env/HW2/cs231n/fast_layers.pyc\u001b[0m in \u001b[0;36mconv_forward_strides\u001b[0;34m(x, w, b, conv_param)\u001b[0m\n\u001b[1;32m     65\u001b[0m   x_stride = np.lib.stride_tricks.as_strided(x_padded,\n\u001b[1;32m     66\u001b[0m                 shape=shape, strides=strides)\n\u001b[0;32m---> 67\u001b[0;31m   \u001b[0mx_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_stride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0mx_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mHH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mWW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout_h\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ertugrulcan46/anaconda2/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36mascontiguousarray\u001b[0;34m(a, dtype)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \"\"\"\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = init_my_convnet(filter_size=5, num_filters=[64, 64, 128, 128, 128], num_crp=4, num_affine_layer=1, num_crcrp=0)\n",
    "trainer = ClassifierTrainer()\n",
    "best_model, loss_history, train_acc_history, val_acc_history = trainer.train(\n",
    "          X_train, y_train, X_val, y_val, model, my_convnet_imp,\n",
    "          reg=0.001, momentum=0.9, learning_rate=0.0005, batch_size=50, num_epochs=5,\n",
    "          acc_frequency=50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = my_convnet_imp(X_test, best_model).argmax(axis=1)\n",
    "test_acc = np.mean(y_test == test_predictions)\n",
    "print test_acc\n",
    "best_model_3 = best_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_my_convnet(filter_size=5, num_filters=[64, 64, 128, 128, 128], num_crp=2, num_affine_layer=1, num_crcrp=2)\n",
    "trainer = ClassifierTrainer()\n",
    "best_model, loss_history, train_acc_history, val_acc_history = trainer.train(\n",
    "          X_train, y_train, X_val, y_val, model, my_convnet_imp,\n",
    "          reg=0.001, momentum=0.9, learning_rate=0.0005, batch_size=50, num_epochs=5,\n",
    "          acc_frequency=50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = my_convnet_imp(X_test, best_model).argmax(axis=1)\n",
    "test_acc = np.mean(y_test == test_predictions)\n",
    "print test_acc\n",
    "best_model_4 = best_model.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried SVM but it did not converged and also it is not better for this task. I have used different architectures. Mainly [Convolution-ReLu-MaxPooling] stack with affine layers at the end. I have also tried the same architecture with conv-relu part repeated. I was going to try batchnorm layer as well but I did not had time since training this took long time because of my PC(chromebook). Increasing the layer number did not worked well. Either there wasn't enough epochs to converge -I think this is the reason- or the problem was not too complicated for such a network. Overall, [Convolution-ReLu-MaxPooling]x2 + [Affine]x1(100neurons) seems to be working the best and when given more epochs it probably increase in terms of accuracy.\n",
    "\n",
    "Accuracy on \"test\" set for this model: 0.647\n",
    "Accuracy on \"validation\" set for this model: 0.65\n",
    "Accuracy on \"training\" set for this model: 0.921"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
